{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This tutorial is Largely based on the paper Context-aware Argumentative Relation Mining \n",
    "Huy V. Nguyen\n",
    "Diane J. Litman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However the implementation is not the exact procedure stated in the paper, rather covers the overall intention.\n",
    "The main intention being somehow give the contextual information to the classifier model by extracting topics using LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation is divided into 6 parts :\n",
    "    \n",
    "    Part 1 : Importing and structuring the Dataset \n",
    "    Part 2 : Window Context Extraction ( TODO )\n",
    "    Part 3 : LDA topic Extraction ( TODO )\n",
    "    Part 4 : Creating and Adding the features ( TODO )\n",
    "    Part 5 : Applying Classification Models ( TODO )\n",
    "    Part 6 : Hyperparameter tuning ( additional )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Library Installation cells. Uncomment them and run the cells. To be skipped if already installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install imblearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel,phrases,Phrases\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1.\n",
    "## Importing and structuring the Dataset of 90 Persuasive ESSAYs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the actual essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_dict = {}\n",
    "for filename in os.listdir(\"brat-project\"):\n",
    "    if filename.endswith(\".txt\"): \n",
    "        \n",
    "        filepath= os.path.join(\"brat-project\", filename)\n",
    "        #print(filepath,'\\n')\n",
    "        try:\n",
    "            file =open(filepath,'r')\n",
    "            essay = file.read()\n",
    "            essay_dict.update({filename[:-4]:sent_tokenize(essay)})\n",
    "            \n",
    "        except IOError as err:\n",
    "            print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_dataset(ann,essay):\n",
    "    '''\n",
    "        This function restructures the ADU segments such that the dataset contains all\n",
    "        possible pairs of source - target permutations and the relation they share\n",
    "    '''\n",
    "    \n",
    "    ann_ADU = ann[ann['ID'].str.startswith('T')] # this gets each ADU IDs\n",
    "    ann_ADU_relations = ann[ann['ID'].str.startswith('R')][['ID','TYPE']] # this gets the relations between the ADU IDs\n",
    "\n",
    "    \n",
    "    # this creates a dataframe of ADU ID, segment, type and ADU start and end positions\n",
    "    ann_ADU= pd.DataFrame(ann_ADU.apply(lambda x : \n",
    "                            list((x['ID'],)+(x['ADU'],) + tuple(x['TYPE'].split(' '))),axis=1).values.tolist(),\n",
    "                          columns = ['ID','ADU','TYPE','strt','end']) \n",
    "\n",
    "    \n",
    "    \n",
    "    # this creates permutation of all ADUs with each other to form a source-target pair\n",
    "    ann_ADU = pd.DataFrame([row[0]+ row[1] for row in itertools.permutations(\n",
    "                                        ann_ADU[['ID','ADU','TYPE','strt','end']].values.tolist(), 2) ],\n",
    "                            columns=['src_id','src','src_type','src_strt','src_end',\n",
    "                                     'tgt_id','tgt','tgt_type','tgt_strt','tgt_end']) \n",
    "\n",
    "    # this creates a dataframe of ADU IDs and their relations\n",
    "    ann_ADU_relations= pd.DataFrame(ann_ADU_relations.apply(lambda x : [x['TYPE'].split(' ')[0],\n",
    "                                         x['TYPE'].split(' ')[1].split(':')[1],\n",
    "                                         x['TYPE'].split(' ')[2].split(':')[1]] ,axis=1).values.tolist(), \n",
    "                 columns=['relation','src_id','tgt_id'])\n",
    "\n",
    "\n",
    "    # finally the above two dataframes are joined together\n",
    "    rearranged_ann = pd.merge(ann_ADU,ann_ADU_relations,on=['src_id','tgt_id'],how='outer')\n",
    "    rearranged_ann.fillna('no relation',inplace=True)\n",
    "    rearranged_ann['essay'] = essay\n",
    "    \n",
    "    return rearranged_ann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame()\n",
    "for filename in os.listdir(\"brat-project\"):\n",
    "    if filename.endswith(\".ann\"): \n",
    "        \n",
    "        filepath= os.path.join(\"brat-project\", filename)\n",
    "        #print(filepath,'\\n')\n",
    "        try:\n",
    "            annotation = pd.read_table(filepath,header=None)\n",
    "            annotation.columns =  ['ID','TYPE','ADU']\n",
    "            dataset = dataset.append(rearrange_dataset(annotation,filename[:-4]))\n",
    "        except pd.errors.ParserError as err:\n",
    "            print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset['relation'] != 'no relation']['relation'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keeping only the 'supports' and 'attacks' relation in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.reset_index(drop=True,inplace=True)\n",
    "dataset_model2 = dataset.drop(dataset[dataset['relation'] == 'no relation'].index, axis=0)\n",
    "dataset_model2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2.\n",
    "## Context Window Extraction\n",
    "\n",
    "\n",
    "### TASK :\n",
    "#### Implement a Loop for tokenizing each sentence contained in variable 'sent' and store it in 'sentToken' dictionary\n",
    "       \n",
    "       For example for some list of sentences in 'sent', 1 entry in the dictionary should look like :  \n",
    "   \n",
    "             { 6 : ['In', 'a', 'word', ',', 'the', 'notion', 'of', 'being', 'afraid',\n",
    "                   'of', 'social', 'misleading', 'is', 'unjustified', '.'] }\n",
    "    \n",
    "       where key '6' refers to the sentence number in the list and the value is the tokenized sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentNeigh(sent, segment,prefix, num_neighbours = 0): \n",
    "    '''\n",
    "        This function gets the neighbouring window sentences(both previous and next) of the \n",
    "        current ADU segment, with a parameter (num_neighbours) to control the window size.\n",
    "    '''\n",
    "    \n",
    "    sentToken = dict() \n",
    "    \n",
    "    ##### YOUR CODE HERE ######\n",
    "\n",
    "    ###########################\n",
    "              \n",
    "\n",
    "    # tokenize the ADU segment\n",
    "    wordList = word_tokenize(segment)\n",
    "    res=-1\n",
    "\n",
    "    # Then Check in every Sentence \n",
    "    for s in range(len(sentToken)): \n",
    "        wCount = len(wordList) \n",
    "\n",
    "        # Every word in the Phrase \n",
    "        for w in wordList: \n",
    "            if w in sentToken[s]: \n",
    "                wCount -= 1\n",
    "\n",
    "        # If every word in phrase matches \n",
    "        if wCount == 0: \n",
    "            res= s \n",
    "            break\n",
    "\n",
    "    if(res > -1): \n",
    "\n",
    "        ret_dict= {}\n",
    "\n",
    "        for i in range(1,num_neighbours+1):\n",
    "\n",
    "            ret_dict.update( {prefix+'_prev_sent'+str(i):[sent[res-i]]})\n",
    "\n",
    "            # to check if the segment has neighbours left on its right.\n",
    "            if res+i < len(sent):\n",
    "                ret_dict.update( {prefix+'_next_sent'+str(i):[sent[res+i]]})\n",
    "\n",
    "        return ret_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting then neighbouring sentences for each source and target pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40 - 60 secs atleast\n",
    "\n",
    "window_size = 4\n",
    "\n",
    "dataset_model2.reset_index(drop=True,inplace=True)\n",
    "neighbours = pd.DataFrame()\n",
    "\n",
    "for row in dataset_model2[['src','tgt','essay']].iterrows():\n",
    "    \n",
    "    essay_num = row[1]['essay']\n",
    "    essay = essay_dict[essay_num]\n",
    "    \n",
    "    src_adu = row[1]['src']\n",
    "    tgt_adu = row[1]['tgt']\n",
    "    \n",
    "    src = pd.DataFrame(getSentNeigh(sent = essay, \n",
    "                                    segment = src_adu ,\n",
    "                                    prefix = 'src', \n",
    "                                    num_neighbours = window_size))\n",
    "    \n",
    "    tgt = pd.DataFrame(getSentNeigh(sent = essay, \n",
    "                                    segment = tgt_adu ,\n",
    "                                    prefix = 'tgt', \n",
    "                                    num_neighbours = window_size))\n",
    "    \n",
    "    neighbours = neighbours.append(pd.concat([src,tgt],axis=1))\n",
    "    \n",
    "neighbours.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the neighbours extracted to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours = neighbours.fillna('')\n",
    "dataset_model2  = pd.concat([dataset_model2,neighbours],axis=1)\n",
    "dataset_model2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3.\n",
    "## LDA Topic Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the extra Essay corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "\n",
    "for filename in os.listdir(\"developemental_data\"):\n",
    "    if filename.endswith(\".txt\"): \n",
    "        filepath= os.path.join(\"developemental_data\", filename)\n",
    "        #print(filepath,'\\n')\n",
    "        try:\n",
    "            file =open(filepath,'r')\n",
    "            data.append(file.readline())\n",
    "            \n",
    "        except IOError as err:\n",
    "            print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: \n",
    "#### Use the re library to remove single quotes and new line characters from each headings of all essays stored in variable data in previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE HERE ######\n",
    "\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text preprocessing functions to tokenize, lemmatize and create bigrams or trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK:\n",
    "#### Implement the function to remove stopwords from each sentence contained in the texts variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    \n",
    "    ##### YOUR CODE HERE ######\n",
    "\n",
    "    ###########################\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK : \n",
    "#### Complete the lemmatization function which checks for Parts of Speech tag to lemmatize each token. \n",
    "#### Hint : \n",
    "#### 1. Check the pos tag to identify if the token is a noun ('NN') or verb ('VB') or adjective ('JJ')\n",
    "#### 2. Lemmatize each token by calling : lemmatizer.lemmatize(token, pos='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pos_tagged_text = [pos_tag(text) for text in texts]\n",
    "    \n",
    "    texts_lemmatized =[]\n",
    "    for text in pos_tagged_text:\n",
    "        text_lemmatized = []\n",
    "        for token, tag in text:\n",
    "            ##### YOUR CODE HERE ######\n",
    "\n",
    "            ###########################\n",
    "                \n",
    "            text_lemmatized.append(token)\n",
    "        texts_lemmatized.append(text_lemmatized)\n",
    "    \n",
    "    return texts_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    # Build the bigram models\n",
    "    bigram = Phrases(texts, min_count=2, threshold=1) # higher threshold fewer phrases.\n",
    "    \n",
    "    bigram_mod = phrases.Phraser(bigram)\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    # Build the bigram and trigram models\n",
    "    bigram = Phrases(texts, min_count=2, threshold=1) # higher threshold fewer phrases.\n",
    "    trigram = Phrases(bigram[texts], min_count=2, threshold=1)  \n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = phrases.Phraser(bigram)\n",
    "    trigram_mod = phrases.Phraser(trigram)\n",
    "    \n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline(data):\n",
    "    \n",
    "    # sentence tokenize\n",
    "    data_words = list(sent_to_words(data))\n",
    "    \n",
    "    # Remove Stop Words\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "    # Do lemmatization\n",
    "    data_lemmatized = lemmatization(data_words_nostops)\n",
    "    \n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(data_lemmatized)\n",
    "\n",
    "     # Form Trigrams\n",
    "    data_words_trigrams = make_trigrams(data_lemmatized)\n",
    "    \n",
    "    return data_words_trigrams\n",
    "\n",
    "print(preprocess_pipeline(data)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(preprocess_pipeline(data))\n",
    "\n",
    "# Create Corpus\n",
    "texts = preprocess_pipeline(data)\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=15, \n",
    "                                           random_state=100,\n",
    "                                           update_every=0,\n",
    "                                           chunksize=10,\n",
    "                                           passes=100,\n",
    "                                           alpha='auto',\n",
    "                                           eta='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "os.environ['MALLET_HOME'] = 'C:\\\\Users\\\\Arkajit\\\\Anaconda2\\\\envs\\\\TFENV\\\\mallet-2.0.8'\n",
    "\n",
    "lda_model = gensim.models.wrappers.LdaMallet('C:\\\\Users\\\\Arkajit\\\\Anaconda2\\\\envs\\\\TFENV\\\\mallet-2.0.8\\\\bin\\\\mallet.bat',\n",
    "                                            corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=36, \n",
    "                                            iterations=100\n",
    "                                            )\n",
    "                                            \n",
    "'''                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3,mallet = False):\n",
    "    '''\n",
    "        This function iterates the ldamodel over a series of different counts of topics and \n",
    "        stores its coherence Score\n",
    "    '''\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        if ( mallet == False):\n",
    "            model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=100,\n",
    "                                           passes=100,\n",
    "                                           alpha='auto',\n",
    "                                           eta='auto')\n",
    "        \n",
    "        else :\n",
    "            model = gensim.models.wrappers.LdaMallet('C:\\\\Users\\\\Arkajit\\\\Anaconda2\\\\envs\\\\TFENV\\\\mallet-2.0.8\\\\bin\\\\mallet.bat',\n",
    "                                           corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics,\n",
    "                                           iterations=100\n",
    "                                           )\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, \n",
    "                                                        corpus=corpus, \n",
    "                                                        texts=texts, \n",
    "                                                        start=2, \n",
    "                                                        limit=40, \n",
    "                                                        step=6,\n",
    "                                                        mallet=True)\n",
    "\n",
    "\n",
    "\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend(\"coherence_values\", loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4.\n",
    "## Creating and adding Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = [[word[0] for word in lda_model.show_topic(i,topn= 50)] for i in range(lda_model.num_topics)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Topic words common between source and target ADUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task : \n",
    "#### Get the number of common tokens between the topic_words and src_ADU and tgt_ADU and store it it in context_words_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_ADU = preprocess_pipeline(dataset_model2['src'].values)\n",
    "tgt_ADU = preprocess_pipeline(dataset_model2['tgt'].values)\n",
    "\n",
    "context_words_lda = []\n",
    "column_names = []\n",
    "\n",
    "for i, topic in enumerate(topic_words):\n",
    "    \n",
    "    ##### YOUR CODE HERE ######\n",
    "\n",
    "    ###########################\n",
    "    column_names.append('topic_'+str(i)+'_src')\n",
    "    column_names.append('topic_'+str(i)+'_tgt')\n",
    "    \n",
    "context_words_lda = pd.DataFrame(np.array(context_words_lda).T, columns = column_names )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Common tokens between source and target ADUs and 8-window context sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runtime: depending on window size atleast 30 secs\n",
    "\n",
    "common_neigh=pd.DataFrame()\n",
    "for col in neighbours.columns:\n",
    "    \n",
    "    if 'src' in col:\n",
    "        temp = pd.DataFrame([len(np.intersect1d(simple_preprocess(str(sent[1]), deacc=True),\n",
    "                                                simple_preprocess(str(sent[0]), deacc=True))) \n",
    "                             for sent in dataset_model2[['src',col]].values],\n",
    "                             columns = [col+'_comm'])\n",
    "    elif 'tgt' in col:\n",
    "        temp = pd.DataFrame([len(np.intersect1d(simple_preprocess(str(sent[1]), deacc=True),\n",
    "                                                simple_preprocess(str(sent[0]), deacc=True))) \n",
    "                             for sent in dataset_model2[['tgt',col]].values],\n",
    "                             columns = [col+'_comm'])\n",
    "    \n",
    "    common_neigh = pd.concat([common_neigh,temp],axis=1)\n",
    "\n",
    "column_names.extend(common_neigh.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 3. Word counts of source and target ADUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_src = [len(sent) for sent in dataset_model2['src'].values ]\n",
    "word_count_tgt = [len(sent) for sent in dataset_model2['tgt'].values ]\n",
    "\n",
    "word_counts = [word_count_src, word_count_tgt ] \n",
    "word_counts = pd.DataFrame(np.array(word_counts).T, columns = ['word_count_src', 'word_count_tgt' ] )\n",
    "\n",
    "column_names.extend( ['word_count_src', 'word_count_tgt' ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Boolean features of source and target ADU types (premise, claim and major claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tgt_type= pd.get_dummies(dataset_model2[['src_type','tgt_type']])\n",
    "\n",
    "column_names.extend(src_tgt_type.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Absolute differences between the positions of the source and target ADUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_diff_strt = pd.DataFrame(np.abs(dataset_model2['tgt_strt'].astype('int') - dataset_model2['src_strt'].astype('int')),\n",
    "                             columns=['abs_diff_strt'])\n",
    "abs_diff_end = pd.DataFrame(np.abs(dataset_model2['tgt_end'].astype('int') -  dataset_model2['src_end'].astype('int')),\n",
    "                            columns=['abs_diff_end'])\n",
    "\n",
    "column_names.extend(['abs_diff_strt','abs_diff_end'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining all the above feature vectors into a training dataset and seperating out the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =pd.concat([src_tgt_type,\n",
    "              abs_diff_strt,\n",
    "              abs_diff_end,\n",
    "              word_counts,\n",
    "              common_neigh,\n",
    "              context_words_lda],axis=1)\n",
    "\n",
    "Y= dataset_model2['relation'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5.\n",
    "## Applying Classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegressionCV , SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.metrics import make_scorer,f1_score , recall_score, precision_score,accuracy_score,precision_recall_fscore_support,cohen_kappa_score,confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV,cross_validate,train_test_split,ShuffleSplit,KFold,StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from imblearn import over_sampling, under_sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Scale the independant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_scaler  = StandardScaler()#Normalizer()#MinMaxScaler()#\n",
    "X = s_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task : \n",
    "#### Split X and Y into training and testing datasets using train_test_split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=[]\n",
    "Y_train=[]\n",
    "X_test=[]\n",
    "Y_test=[]\n",
    "##### YOUR CODE HERE ######\n",
    "\n",
    "###########################\n",
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampler=over_sampling.SVMSMOTE(sampling_strategy= 1)\n",
    "#X_train, Y_train=sampler.fit_resample(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Apply the best Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC()\n",
    "\n",
    "model.fit(X_train,Y_train)\n",
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the precision macro, recall macro, f1 macro and accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p_macro, r_macro, f_macro, support_macro = precision_recall_fscore_support(y_true=Y_test, \n",
    "                                                                           y_pred=Y_pred, \n",
    "                                                                           labels=[0,1], \n",
    "                                                                           average='macro')\n",
    "\n",
    "print('Accuracy:',round(accuracy_score(Y_test, Y_pred),2),\n",
    "      '\\nKappa:',round(cohen_kappa_score(Y_test,Y_pred),2),\n",
    "      '\\nMacro Precision:',round(p_macro,2),\n",
    "      '\\nMacro Recall:', round(r_macro,2),\n",
    "      '\\nMacro F1:',round(f_macro,2,),\n",
    "      '\\nF1:',round(f1_score(Y_test, Y_pred),2,)\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6.\n",
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5)#ShuffleSplit(n_splits=10, test_size=0.25,random_state=0)\n",
    "\n",
    "scoring = ['precision_macro', 'recall_macro','f1_macro']\n",
    "scores = cross_validate(model, X, Y, scoring=scoring, cv=cv)\n",
    "scores['test_precision_macro'].mean(),scores['test_recall_macro'].mean(),scores['test_f1_macro'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score,labels=[0,1],average='macro'),\n",
    "    'recall_score': make_scorer(recall_score,labels=[0,1],average='macro'),\n",
    "    'f1_score': make_scorer(f1_score,labels=[0,1],average='macro')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    "    'C':  [c for c in range(1,120,2)]\n",
    "}\n",
    "\n",
    "clf = SVC(class_weight='balanced')#GradientBoostingClassifier(random_state = 42)#RandomForestClassifier(class_weight='balanced_subsample')#\n",
    "\n",
    "gsearch1 = GridSearchCV(\n",
    "    n_jobs=-1,\n",
    "    estimator=clf, \n",
    "    param_grid=param_test1,\n",
    "    scoring= scorers,#'precision_macro',\n",
    "    verbose= True,\n",
    "    iid=True,\n",
    "    refit='precision_score',\n",
    "    #n_iter= 100,\n",
    "    cv=KFold(n_splits=5))\n",
    "\n",
    "gsearch1.fit(X_train, Y_train)\n",
    "\n",
    "gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(gsearch1.cv_results_)[[\n",
    "       #'param_max_depth', 'param_n_estimators', \n",
    "      'params',\n",
    "       'rank_test_precision_score',\n",
    "       'rank_test_recall_score', \n",
    "       'rank_test_f1_score', \n",
    "        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import rv_discrete\n",
    "\n",
    "depths = rv_discrete(values= ([i for i in range(1,6,1)], [0.2]*5))\n",
    "n_iterations = rv_discrete(values = ([i for i in range(100,2100,50)],[0.025]*40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    "    \"n_estimators\":n_iterations,\n",
    "    'max_depth': depths,\n",
    "}\n",
    "\n",
    "clf = GradientBoostingClassifier(random_state = 42)#RandomForestClassifier(class_weight='balanced_subsample')#SVC()\n",
    "\n",
    "gsearch1 = RandomizedSearchCV(\n",
    "    n_jobs=-1,\n",
    "    estimator=clf, \n",
    "    param_distributions=param_test1,\n",
    "    scoring= scorers,#'precision_macro',\n",
    "    verbose= True,\n",
    "    iid=True,\n",
    "    refit='precision_score',\n",
    "    n_iter= 100,\n",
    "    cv=cv)\n",
    "\n",
    "gsearch1.fit(X, Y)\n",
    "\n",
    "gsearch1.best_params_, gsearch1.best_score_"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
