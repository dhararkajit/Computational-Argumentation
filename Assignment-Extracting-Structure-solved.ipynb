{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This tutorial is Largely based on the paper Context-aware Argumentative Relation Mining \n",
    "Huy V. Nguyen\n",
    "Diane J. Litman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However the implementation is not the exact procedure stated in the paper, rather covers the overall intention.\n",
    "The main intention being somehow give the contextual information to the classifier model by extracting topics using LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation is divided into 6 parts :\n",
    "    \n",
    "    Part 1 : Importing and structuring the Dataset \n",
    "    Part 2 : Window Context Extraction ( TODO )\n",
    "    Part 3 : LDA topic Extraction ( TODO )\n",
    "    Part 4 : Creating and Adding the features ( TODO )\n",
    "    Part 5 : Applying Classification Models ( TODO )\n",
    "    Part 6 : Hyperparameter tuning ( additional )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Library Installation cells. Uncomment them and run the cells. To be skipped if already installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install imblearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel,phrases,Phrases\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1.\n",
    "## Importing and structuring the Dataset of 90 Persuasive ESSAYs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the actual essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_dict = {}\n",
    "for filename in os.listdir(\"brat-project\"):\n",
    "    if filename.endswith(\".txt\"): \n",
    "        \n",
    "        filepath= os.path.join(\"brat-project\", filename)\n",
    "        #print(filepath,'\\n')\n",
    "        try:\n",
    "            file =open(filepath,'r')\n",
    "            essay = file.read()\n",
    "            essay_dict.update({filename[:-4]:sent_tokenize(essay)})\n",
    "            \n",
    "        except IOError as err:\n",
    "            print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Study at school or get a job?',\n",
       " 'Many people believe that children should study at school to have more knowledge that prepare better for their future.',\n",
       " 'Others, however, think that these children may disrupt their school work and should be allowed to leave school early to find a job.',\n",
       " 'Personally, I tend to agree with the point of view that student have to be forced to study at school.',\n",
       " 'First of all, schools offer to students a good environment with experienced professors and high quality programs for studying.',\n",
       " 'It creates the best conditions for students education and can force them to focus on their school work instead of wasting their time to do useless things.',\n",
       " 'Second of all, schools provide lots of academic knowledge to students.',\n",
       " 'Students may learn professional skills, expand their understandings and gain experiences.',\n",
       " 'Therefore, they have more opprotunities to find a job and to be successful in the future.',\n",
       " 'For example, as we know, employer always prefer to hire an employee of high degree who have professional skills.',\n",
       " 'Nevertheless, it is not unreasonable that some people think that children should interrupt their school work and get a job.',\n",
       " 'Whether children can learn a lot at school, there are many subjects that will be of little value to them in the future.',\n",
       " 'Furthermore, children can learn social skills when they have a job.',\n",
       " 'They can get more experiences that can not be obtained at school.',\n",
       " \"Working helps children be more independent and teach them to esteem and manage the money that they've earned.\",\n",
       " 'Overall, I believe that students should study at school.',\n",
       " \"Even though there are some advantages of leaving school to find a job, studying at school is always the best choice for children's future.\",\n",
       " 'There are many ways that can train children to learn independent and social skills instead of getting a job.']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essay_dict['essay34']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_dataset(ann,essay):\n",
    "    '''\n",
    "        This function restructures the ADU segments such that the dataset contains all\n",
    "        possible pairs of source - target permutations and the relation they share\n",
    "    '''\n",
    "    \n",
    "    ann_ADU = ann[ann['ID'].str.startswith('T')] # this gets each ADU IDs\n",
    "    ann_ADU_relations = ann[ann['ID'].str.startswith('R')][['ID','TYPE']] # this gets the relations between the ADU IDs\n",
    "\n",
    "    \n",
    "    # this creates a dataframe of ADU ID, segment, type and ADU start and end positions\n",
    "    ann_ADU= pd.DataFrame(ann_ADU.apply(lambda x : \n",
    "                            list((x['ID'],)+(x['ADU'],) + tuple(x['TYPE'].split(' '))),axis=1).values.tolist(),\n",
    "                          columns = ['ID','ADU','TYPE','strt','end']) \n",
    "\n",
    "    \n",
    "    \n",
    "    # this creates permutation of all ADUs with each other to form a source-target pair\n",
    "    ann_ADU = pd.DataFrame([row[0]+ row[1] for row in itertools.permutations(\n",
    "                                        ann_ADU[['ID','ADU','TYPE','strt','end']].values.tolist(), 2) ],\n",
    "                            columns=['src_id','src','src_type','src_strt','src_end',\n",
    "                                     'tgt_id','tgt','tgt_type','tgt_strt','tgt_end']) \n",
    "\n",
    "    # this creates a dataframe of ADU IDs and their relations\n",
    "    ann_ADU_relations= pd.DataFrame(ann_ADU_relations.apply(lambda x : [x['TYPE'].split(' ')[0],\n",
    "                                         x['TYPE'].split(' ')[1].split(':')[1],\n",
    "                                         x['TYPE'].split(' ')[2].split(':')[1]] ,axis=1).values.tolist(), \n",
    "                 columns=['relation','src_id','tgt_id'])\n",
    "\n",
    "\n",
    "    # finally the above two dataframes are joined together\n",
    "    rearranged_ann = pd.merge(ann_ADU,ann_ADU_relations,on=['src_id','tgt_id'],how='outer')\n",
    "    rearranged_ann.fillna('no relation',inplace=True)\n",
    "    rearranged_ann['essay'] = essay\n",
    "    \n",
    "    return rearranged_ann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame()\n",
    "for filename in os.listdir(\"brat-project\"):\n",
    "    if filename.endswith(\".ann\"): \n",
    "        \n",
    "        filepath= os.path.join(\"brat-project\", filename)\n",
    "        #print(filepath,'\\n')\n",
    "        try:\n",
    "            annotation = pd.read_table(filepath,header=None)\n",
    "            annotation.columns =  ['ID','TYPE','ADU']\n",
    "            dataset = dataset.append(rearrange_dataset(annotation,filename[:-4]))\n",
    "        except pd.errors.ParserError as err:\n",
    "            print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_id</th>\n",
       "      <th>src</th>\n",
       "      <th>src_type</th>\n",
       "      <th>src_strt</th>\n",
       "      <th>src_end</th>\n",
       "      <th>tgt_id</th>\n",
       "      <th>tgt</th>\n",
       "      <th>tgt_type</th>\n",
       "      <th>tgt_strt</th>\n",
       "      <th>tgt_end</th>\n",
       "      <th>relation</th>\n",
       "      <th>essay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1</td>\n",
       "      <td>competition can effectively promote the develo...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>78</td>\n",
       "      <td>140</td>\n",
       "      <td>T2</td>\n",
       "      <td>we should attach more importance to cooperation</td>\n",
       "      <td>MajorClaim</td>\n",
       "      <td>503</td>\n",
       "      <td>550</td>\n",
       "      <td>attacks</td>\n",
       "      <td>essay01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T1</td>\n",
       "      <td>competition can effectively promote the develo...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>78</td>\n",
       "      <td>140</td>\n",
       "      <td>T3</td>\n",
       "      <td>In order to survive in the competition, compan...</td>\n",
       "      <td>Premise</td>\n",
       "      <td>142</td>\n",
       "      <td>283</td>\n",
       "      <td>no relation</td>\n",
       "      <td>essay01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T1</td>\n",
       "      <td>competition can effectively promote the develo...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>78</td>\n",
       "      <td>140</td>\n",
       "      <td>T4</td>\n",
       "      <td>through cooperation, children can learn about ...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>591</td>\n",
       "      <td>714</td>\n",
       "      <td>no relation</td>\n",
       "      <td>essay01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T1</td>\n",
       "      <td>competition can effectively promote the develo...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>78</td>\n",
       "      <td>140</td>\n",
       "      <td>T5</td>\n",
       "      <td>What we acquired from team work is not only ho...</td>\n",
       "      <td>Premise</td>\n",
       "      <td>716</td>\n",
       "      <td>851</td>\n",
       "      <td>no relation</td>\n",
       "      <td>essay01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T1</td>\n",
       "      <td>competition can effectively promote the develo...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>78</td>\n",
       "      <td>140</td>\n",
       "      <td>T6</td>\n",
       "      <td>During the process of cooperation, children ca...</td>\n",
       "      <td>Premise</td>\n",
       "      <td>853</td>\n",
       "      <td>1086</td>\n",
       "      <td>no relation</td>\n",
       "      <td>essay01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  src_id                                                src src_type src_strt  \\\n",
       "0     T1  competition can effectively promote the develo...    Claim       78   \n",
       "1     T1  competition can effectively promote the develo...    Claim       78   \n",
       "2     T1  competition can effectively promote the develo...    Claim       78   \n",
       "3     T1  competition can effectively promote the develo...    Claim       78   \n",
       "4     T1  competition can effectively promote the develo...    Claim       78   \n",
       "\n",
       "  src_end tgt_id                                                tgt  \\\n",
       "0     140     T2    we should attach more importance to cooperation   \n",
       "1     140     T3  In order to survive in the competition, compan...   \n",
       "2     140     T4  through cooperation, children can learn about ...   \n",
       "3     140     T5  What we acquired from team work is not only ho...   \n",
       "4     140     T6  During the process of cooperation, children ca...   \n",
       "\n",
       "     tgt_type tgt_strt tgt_end     relation    essay  \n",
       "0  MajorClaim      503     550      attacks  essay01  \n",
       "1     Premise      142     283  no relation  essay01  \n",
       "2       Claim      591     714  no relation  essay01  \n",
       "3     Premise      716     851  no relation  essay01  \n",
       "4     Premise      853    1086  no relation  essay01  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "supports    1312\n",
       "attacks      161\n",
       "Name: relation, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset['relation'] != 'no relation']['relation'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keeping only the 'supports' and 'attacks' relation in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1473, 12)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.reset_index(drop=True,inplace=True)\n",
    "dataset_model2 = dataset.drop(dataset[dataset['relation'] == 'no relation'].index, axis=0)\n",
    "dataset_model2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2.\n",
    "## Context Window Extraction\n",
    "\n",
    "\n",
    "### TASK :\n",
    "#### Implement a Loop for tokenizing each sentence contained in variable 'sent' and store it in 'sentToken' dictionary\n",
    "       \n",
    "       For example for some list of sentences in 'sent', 1 entry in the dictionary should look like :  \n",
    "   \n",
    "             { 6 : ['In', 'a', 'word', ',', 'the', 'notion', 'of', 'being', 'afraid',\n",
    "                   'of', 'social', 'misleading', 'is', 'unjustified', '.'] }\n",
    "    \n",
    "       where key '6' refers to the sentence number in the list and the value is the tokenized sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentNeigh(sent, segment,prefix, num_neighbours = 0): \n",
    "    '''\n",
    "        This function gets the neighbouring window sentences(both previous and next) of the \n",
    "        current ADU segment, with a parameter (num_neighbours) to control the window size.\n",
    "    '''\n",
    "    \n",
    "    sentToken = dict() \n",
    "    \n",
    "    # Loop for tokenizing each sentence\n",
    "    for s in range(len(sent)): \n",
    "        sentToken[s] = word_tokenize(sent[s])\n",
    "              \n",
    "\n",
    "    # tokenize the ADU segment\n",
    "    wordList = word_tokenize(segment)\n",
    "    res=-1\n",
    "\n",
    "    # Then Check in every Sentence \n",
    "    for s in range(len(sentToken)): \n",
    "        wCount = len(wordList) \n",
    "\n",
    "        # Every word in the Phrase \n",
    "        for w in wordList: \n",
    "            if w in sentToken[s]: \n",
    "                wCount -= 1\n",
    "\n",
    "        # If every word in phrase matches \n",
    "        if wCount == 0: \n",
    "            res= s \n",
    "            break\n",
    "\n",
    "    if(res > -1): \n",
    "\n",
    "        ret_dict= {}\n",
    "\n",
    "        for i in range(1,num_neighbours+1):\n",
    "\n",
    "            ret_dict.update( {prefix+'_prev_sent'+str(i):[sent[res-i]]})\n",
    "\n",
    "            # to check if the segment has neighbours left on its right.\n",
    "            if res+i < len(sent):\n",
    "                ret_dict.update( {prefix+'_next_sent'+str(i):[sent[res+i]]})\n",
    "\n",
    "        return ret_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting then neighbouring sentences for each source and target pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40 - 60 secs atleast\n",
    "\n",
    "window_size = 4\n",
    "\n",
    "dataset_model2.reset_index(drop=True,inplace=True)\n",
    "neighbours = pd.DataFrame()\n",
    "\n",
    "for row in dataset_model2[['src','tgt','essay']].iterrows():\n",
    "    \n",
    "    essay_num = row[1]['essay']\n",
    "    essay = essay_dict[essay_num]\n",
    "    \n",
    "    src_adu = row[1]['src']\n",
    "    tgt_adu = row[1]['tgt']\n",
    "    \n",
    "    src = pd.DataFrame(getSentNeigh(sent = essay, \n",
    "                                    segment = src_adu ,\n",
    "                                    prefix = 'src', \n",
    "                                    num_neighbours = window_size))\n",
    "    \n",
    "    tgt = pd.DataFrame(getSentNeigh(sent = essay, \n",
    "                                    segment = tgt_adu ,\n",
    "                                    prefix = 'tgt', \n",
    "                                    num_neighbours = window_size))\n",
    "    \n",
    "    neighbours = neighbours.append(pd.concat([src,tgt],axis=1))\n",
    "    \n",
    "neighbours.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the neighbours extracted to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1473, 28)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbours = neighbours.fillna('')\n",
    "dataset_model2  = pd.concat([dataset_model2,neighbours],axis=1)\n",
    "dataset_model2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3.\n",
    "## LDA Topic Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the extra Essay corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "\n",
    "for filename in os.listdir(\"developemental_data\"):\n",
    "    if filename.endswith(\".txt\"): \n",
    "        filepath= os.path.join(\"developemental_data\", filename)\n",
    "        #print(filepath,'\\n')\n",
    "        try:\n",
    "            file =open(filepath,'r')\n",
    "            data.append(file.readline())\n",
    "            \n",
    "        except IOError as err:\n",
    "            print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Should students be taught to compete or to cooperate? ',\n",
       " 'More people are migrating to other countries than ever before ',\n",
       " 'International tourism is now more common than ever before ',\n",
       " 'International tourism is now more common than ever before ',\n",
       " 'Living and studying overseas ']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: \n",
    "#### Use the re library to remove single quotes and new line characters from each headings of all essays stored in variable data in previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text preprocessing functions to tokenize, lemmatize and create bigrams or trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK:\n",
    "#### Implement the function to remove stopwords from each sentence contained in the texts variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    \n",
    "    return [[word for word in doc if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK : \n",
    "#### Complete the lemmatization function which checks for Parts of Speech tag to lemmatize each token. \n",
    "#### Hint : \n",
    "#### 1. Check the pos tag to identify if the token is a noun ('NN') or verb ('VB') or adjective ('JJ')\n",
    "#### 2. Lemmatize each token by calling : lemmatizer.lemmatize(token, pos='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pos_tagged_text = [pos_tag(text) for text in texts]\n",
    "    \n",
    "    texts_lemmatized =[]\n",
    "    for text in pos_tagged_text:\n",
    "        text_lemmatized = []\n",
    "        for token, tag in text:\n",
    "            if tag.startswith(\"NN\"):\n",
    "                token = lemmatizer.lemmatize(token, pos='n')\n",
    "            elif tag.startswith('VB'):\n",
    "                token = lemmatizer.lemmatize(token, pos='v')\n",
    "            elif tag.startswith('JJ'):\n",
    "                token = lemmatizer.lemmatize(token, pos='a')\n",
    "                \n",
    "            text_lemmatized.append(token)\n",
    "        texts_lemmatized.append(text_lemmatized)\n",
    "    \n",
    "    return texts_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    # Build the bigram models\n",
    "    bigram = Phrases(texts, min_count=2, threshold=1) # higher threshold fewer phrases.\n",
    "    \n",
    "    bigram_mod = phrases.Phraser(bigram)\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    # Build the bigram and trigram models\n",
    "    bigram = Phrases(texts, min_count=2, threshold=1) # higher threshold fewer phrases.\n",
    "    trigram = Phrases(bigram[texts], min_count=2, threshold=1)  \n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = phrases.Phraser(bigram)\n",
    "    trigram_mod = phrases.Phraser(trigram)\n",
    "    \n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['student', 'teach', 'compete', 'cooperate'], ['people', 'migrate', 'country', 'ever'], ['international_tourism', 'common', 'ever'], ['international_tourism', 'common', 'ever'], ['live', 'study', 'overseas'], ['exercise'], ['newspaper', 'become', 'thing', 'past'], ['technology', 'cannot', 'solve', 'world', 'problem'], ['truth', 'cannabis'], ['single', 'international', 'language']]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_pipeline(data):\n",
    "    \n",
    "    # sentence tokenize\n",
    "    data_words = list(sent_to_words(data))\n",
    "    \n",
    "    # Remove Stop Words\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "    # Do lemmatization\n",
    "    data_lemmatized = lemmatization(data_words_nostops)\n",
    "    \n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(data_lemmatized)\n",
    "\n",
    "     # Form Trigrams\n",
    "    data_words_trigrams = make_trigrams(data_lemmatized)\n",
    "    \n",
    "    return data_words_trigrams\n",
    "\n",
    "print(preprocess_pipeline(data)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(25, 1), (26, 1), (27, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(preprocess_pipeline(data))\n",
    "\n",
    "# Create Corpus\n",
    "texts = preprocess_pipeline(data)\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=15, \n",
    "                                           random_state=100,\n",
    "                                           update_every=0,\n",
    "                                           chunksize=10,\n",
    "                                           passes=100,\n",
    "                                           alpha='auto',\n",
    "                                           eta='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "os.environ['MALLET_HOME'] = 'C:\\\\Users\\\\Arkajit\\\\Anaconda2\\\\envs\\\\TFENV\\\\mallet-2.0.8'\n",
    "\n",
    "lda_model = gensim.models.wrappers.LdaMallet('C:\\\\Users\\\\Arkajit\\\\Anaconda2\\\\envs\\\\TFENV\\\\mallet-2.0.8\\\\bin\\\\mallet.bat',\n",
    "                                            corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=36, \n",
    "                                            iterations=100\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.7923088842456681\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3,mallet = False):\n",
    "    '''\n",
    "        This function iterates the ldamodel over a series of different counts of topics and \n",
    "        stores its coherence Score\n",
    "    '''\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        if ( mallet == False):\n",
    "            model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=100,\n",
    "                                           passes=100,\n",
    "                                           alpha='auto',\n",
    "                                           eta='auto')\n",
    "        \n",
    "        else :\n",
    "            model = gensim.models.wrappers.LdaMallet('C:\\\\Users\\\\Arkajit\\\\Anaconda2\\\\envs\\\\TFENV\\\\mallet-2.0.8\\\\bin\\\\mallet.bat',\n",
    "                                           corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics,\n",
    "                                           iterations=100\n",
    "                                           )\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, \n",
    "                                                        corpus=corpus, \n",
    "                                                        texts=texts, \n",
    "                                                        start=2, \n",
    "                                                        limit=40, \n",
    "                                                        step=6,\n",
    "                                                        mallet=True)\n",
    "\n",
    "\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend(\"coherence_values\", loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4.\n",
    "## Creating and adding Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = [[word[0] for word in lda_model.show_topic(i,topn= 50)] for i in range(lda_model.num_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['influence', 'artist', 'improve', 'rich', 'university']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_words[13][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Topic words common between source and target ADUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task : \n",
    "#### Get the number of common tokens between the topic_words and src_ADU and tgt_ADU and store it it in context_words_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_ADU = preprocess_pipeline(dataset_model2['src'].values)\n",
    "tgt_ADU = preprocess_pipeline(dataset_model2['tgt'].values)\n",
    "\n",
    "context_words_lda = []\n",
    "column_names = []\n",
    "\n",
    "for i, topic in enumerate(topic_words):\n",
    "    \n",
    "    context_words_lda.append([ len(np.intersect1d(topic,sent)) for sent in src_ADU])\n",
    "    context_words_lda.append([ len(np.intersect1d(topic,sent)) for sent in tgt_ADU])\n",
    "    column_names.append('topic_'+str(i)+'_src')\n",
    "    column_names.append('topic_'+str(i)+'_tgt')\n",
    "    \n",
    "context_words_lda = pd.DataFrame(np.array(context_words_lda).T, columns = column_names )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Common tokens between source and target ADUs and 8-window context sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runtime: depending on window size atleast 30 secs\n",
    "\n",
    "common_neigh=pd.DataFrame()\n",
    "for col in neighbours.columns:\n",
    "    \n",
    "    if 'src' in col:\n",
    "        temp = pd.DataFrame([len(np.intersect1d(simple_preprocess(str(sent[1]), deacc=True),\n",
    "                                                simple_preprocess(str(sent[0]), deacc=True))) \n",
    "                             for sent in dataset_model2[['src',col]].values],\n",
    "                             columns = [col+'_comm'])\n",
    "    elif 'tgt' in col:\n",
    "        temp = pd.DataFrame([len(np.intersect1d(simple_preprocess(str(sent[1]), deacc=True),\n",
    "                                                simple_preprocess(str(sent[0]), deacc=True))) \n",
    "                             for sent in dataset_model2[['tgt',col]].values],\n",
    "                             columns = [col+'_comm'])\n",
    "    \n",
    "    common_neigh = pd.concat([common_neigh,temp],axis=1)\n",
    "\n",
    "column_names.extend(common_neigh.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 3. Word counts of source and target ADUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_src = [len(sent) for sent in dataset_model2['src'].values ]\n",
    "word_count_tgt = [len(sent) for sent in dataset_model2['tgt'].values ]\n",
    "\n",
    "word_counts = [word_count_src, word_count_tgt ] \n",
    "word_counts = pd.DataFrame(np.array(word_counts).T, columns = ['word_count_src', 'word_count_tgt' ] )\n",
    "\n",
    "column_names.extend( ['word_count_src', 'word_count_tgt' ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Boolean features of source and target ADU types (premise, claim and major claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tgt_type= pd.get_dummies(dataset_model2[['src_type','tgt_type']])\n",
    "\n",
    "column_names.extend(src_tgt_type.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Absolute differences between the positions of the source and target ADUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_diff_strt = pd.DataFrame(np.abs(dataset_model2['tgt_strt'].astype('int') - dataset_model2['src_strt'].astype('int')),\n",
    "                             columns=['abs_diff_strt'])\n",
    "abs_diff_end = pd.DataFrame(np.abs(dataset_model2['tgt_end'].astype('int') -  dataset_model2['src_end'].astype('int')),\n",
    "                            columns=['abs_diff_end'])\n",
    "\n",
    "column_names.extend(['abs_diff_strt','abs_diff_end'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining all the above feature vectors into a training dataset and seperating out the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =pd.concat([src_tgt_type,\n",
    "              abs_diff_strt,\n",
    "              abs_diff_end,\n",
    "              word_counts,\n",
    "              common_neigh,\n",
    "              context_words_lda],axis=1)\n",
    "\n",
    "Y= dataset_model2['relation'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_type_Claim</th>\n",
       "      <th>src_type_Premise</th>\n",
       "      <th>tgt_type_Claim</th>\n",
       "      <th>tgt_type_MajorClaim</th>\n",
       "      <th>tgt_type_Premise</th>\n",
       "      <th>abs_diff_strt</th>\n",
       "      <th>abs_diff_end</th>\n",
       "      <th>word_count_src</th>\n",
       "      <th>word_count_tgt</th>\n",
       "      <th>src_next_sent1_comm</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_31_src</th>\n",
       "      <th>topic_31_tgt</th>\n",
       "      <th>topic_32_src</th>\n",
       "      <th>topic_32_tgt</th>\n",
       "      <th>topic_33_src</th>\n",
       "      <th>topic_33_tgt</th>\n",
       "      <th>topic_34_src</th>\n",
       "      <th>topic_34_tgt</th>\n",
       "      <th>topic_35_src</th>\n",
       "      <th>topic_35_tgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>425</td>\n",
       "      <td>410</td>\n",
       "      <td>62</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>143</td>\n",
       "      <td>141</td>\n",
       "      <td>62</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>164</td>\n",
       "      <td>123</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>137</td>\n",
       "      <td>135</td>\n",
       "      <td>123</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>262</td>\n",
       "      <td>372</td>\n",
       "      <td>233</td>\n",
       "      <td>123</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   src_type_Claim  src_type_Premise  tgt_type_Claim  tgt_type_MajorClaim  \\\n",
       "0               1                 0               0                    1   \n",
       "1               0                 1               1                    0   \n",
       "2               1                 0               0                    1   \n",
       "3               0                 1               1                    0   \n",
       "4               0                 1               1                    0   \n",
       "\n",
       "   tgt_type_Premise  abs_diff_strt  abs_diff_end  word_count_src  \\\n",
       "0                 0            425           410              62   \n",
       "1                 0             64           143             141   \n",
       "2                 0             88           164             123   \n",
       "3                 0            125           137             135   \n",
       "4                 0            262           372             233   \n",
       "\n",
       "   word_count_tgt  src_next_sent1_comm      ...       topic_31_src  \\\n",
       "0              47                    2      ...                  0   \n",
       "1              62                    4      ...                  0   \n",
       "2              47                    1      ...                  0   \n",
       "3             123                    6      ...                  1   \n",
       "4             123                    6      ...                  1   \n",
       "\n",
       "   topic_31_tgt  topic_32_src  topic_32_tgt  topic_33_src  topic_33_tgt  \\\n",
       "0             0             0             0             1             0   \n",
       "1             0             0             0             0             1   \n",
       "2             0             0             0             0             0   \n",
       "3             0             0             0             0             0   \n",
       "4             0             2             0             1             0   \n",
       "\n",
       "   topic_34_src  topic_34_tgt  topic_35_src  topic_35_tgt  \n",
       "0             0             0             0             0  \n",
       "1             0             0             0             0  \n",
       "2             0             0             0             0  \n",
       "3             0             0             0             0  \n",
       "4             1             0             1             0  \n",
       "\n",
       "[5 rows x 97 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5.\n",
    "## Applying Classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegressionCV , SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.metrics import make_scorer,f1_score , recall_score, precision_score,accuracy_score,precision_recall_fscore_support,cohen_kappa_score,confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV,cross_validate,train_test_split,ShuffleSplit,KFold,StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from imblearn import over_sampling, under_sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Scale the independant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_scaler  = StandardScaler()#Normalizer()#MinMaxScaler()#\n",
    "X = s_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task : \n",
    "#### Split X and Y into training and testing datasets using train_test_split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1178, 97) (1178,) (295, 97) (295,)\n"
     ]
    }
   ],
   "source": [
    "X_train ,X_test, Y_train ,  Y_test = train_test_split(X, \n",
    "                                                      Y, \n",
    "                                                      test_size=0.2,\n",
    "                                                      random_state=0)\n",
    "\n",
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampler=over_sampling.SVMSMOTE(sampling_strategy= 0.8)\n",
    "#X_train, Y_train=sampler.fit_resample(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Apply the best Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(C=37,class_weight='balanced')\n",
    "\n",
    "model.fit(X_train,Y_train)\n",
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the precision macro, recall macro, f1 macro and accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.87 \n",
      "Kappa: 0.24 \n",
      "Macro Precision: 0.68 \n",
      "Macro Recall: 0.59 \n",
      "Macro F1: 0.61 \n",
      "F1: 0.93\n"
     ]
    }
   ],
   "source": [
    "p_macro, r_macro, f_macro, support_macro = precision_recall_fscore_support(y_true=Y_test, \n",
    "                                                                           y_pred=Y_pred, \n",
    "                                                                           labels=[0,1], \n",
    "                                                                           average='macro')\n",
    "\n",
    "print('Accuracy:',round(accuracy_score(Y_test, Y_pred),2),\n",
    "      '\\nKappa:',round(cohen_kappa_score(Y_test,Y_pred),2),\n",
    "      '\\nMacro Precision:',round(p_macro,2),\n",
    "      '\\nMacro Recall:', round(r_macro,2),\n",
    "      '\\nMacro F1:',round(f_macro,2,),\n",
    "      '\\nF1:',round(f1_score(Y_test, Y_pred),2,)\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6.\n",
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5108049597020584, 0.5064966760713203, 0.5048243968181988)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = KFold(n_splits=5)#ShuffleSplit(n_splits=10, test_size=0.25,random_state=0)\n",
    "\n",
    "scoring = ['precision_macro', 'recall_macro','f1_macro']\n",
    "scores = cross_validate(model, X, Y, scoring=scoring, cv=cv)\n",
    "scores['test_precision_macro'].mean(),scores['test_recall_macro'].mean(),scores['test_f1_macro'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score,labels=[0,1],average='macro'),\n",
    "    'recall_score': make_scorer(recall_score,labels=[0,1],average='macro'),\n",
    "    'f1_score': make_scorer(f1_score,labels=[0,1],average='macro')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   33.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'C': 37}, 0.7355946222338455)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test1 = {\n",
    "    'C': [c for c in range(1,120,2)]\n",
    "}\n",
    "\n",
    "clf = SVC(class_weight='balanced')#GradientBoostingClassifier(random_state = 42)#RandomForestClassifier(class_weight='balanced_subsample')#\n",
    "\n",
    "gsearch1 = GridSearchCV(\n",
    "    n_jobs=-1,\n",
    "    estimator=clf, \n",
    "    param_grid=param_test1,\n",
    "    scoring= scorers,\n",
    "    verbose= True,\n",
    "    iid=True,\n",
    "    refit='f1_score',\n",
    "    cv=KFold(n_splits=5))\n",
    "\n",
    "gsearch1.fit(X_train, Y_train)\n",
    "\n",
    "gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_precision_score</th>\n",
       "      <th>rank_test_recall_score</th>\n",
       "      <th>rank_test_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'C': 1}</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'C': 3}</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'C': 5}</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'C': 7}</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'C': 9}</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'C': 11}</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'C': 13}</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'C': 15}</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'C': 17}</td>\n",
       "      <td>52</td>\n",
       "      <td>32</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'C': 19}</td>\n",
       "      <td>51</td>\n",
       "      <td>31</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'C': 21}</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'C': 23}</td>\n",
       "      <td>46</td>\n",
       "      <td>10</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'C': 25}</td>\n",
       "      <td>46</td>\n",
       "      <td>10</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'C': 27}</td>\n",
       "      <td>48</td>\n",
       "      <td>13</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'C': 29}</td>\n",
       "      <td>49</td>\n",
       "      <td>14</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'C': 31}</td>\n",
       "      <td>45</td>\n",
       "      <td>9</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'C': 33}</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'C': 35}</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'C': 37}</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'C': 39}</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'C': 41}</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>{'C': 43}</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>{'C': 45}</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>{'C': 47}</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>{'C': 49}</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>{'C': 51}</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>{'C': 53}</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>{'C': 55}</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>{'C': 57}</td>\n",
       "      <td>39</td>\n",
       "      <td>29</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>{'C': 59}</td>\n",
       "      <td>40</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>{'C': 61}</td>\n",
       "      <td>40</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>{'C': 63}</td>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>{'C': 65}</td>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>{'C': 67}</td>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>{'C': 69}</td>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>{'C': 71}</td>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>{'C': 73}</td>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>{'C': 75}</td>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>{'C': 77}</td>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>{'C': 79}</td>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>{'C': 81}</td>\n",
       "      <td>43</td>\n",
       "      <td>35</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>{'C': 83}</td>\n",
       "      <td>43</td>\n",
       "      <td>35</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>{'C': 85}</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>{'C': 87}</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>{'C': 89}</td>\n",
       "      <td>42</td>\n",
       "      <td>38</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>{'C': 91}</td>\n",
       "      <td>25</td>\n",
       "      <td>37</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>{'C': 93}</td>\n",
       "      <td>38</td>\n",
       "      <td>52</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>{'C': 95}</td>\n",
       "      <td>23</td>\n",
       "      <td>50</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>{'C': 97}</td>\n",
       "      <td>23</td>\n",
       "      <td>50</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>{'C': 99}</td>\n",
       "      <td>16</td>\n",
       "      <td>47</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>{'C': 101}</td>\n",
       "      <td>16</td>\n",
       "      <td>47</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>{'C': 103}</td>\n",
       "      <td>16</td>\n",
       "      <td>47</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>{'C': 105}</td>\n",
       "      <td>7</td>\n",
       "      <td>41</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>{'C': 107}</td>\n",
       "      <td>7</td>\n",
       "      <td>41</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>{'C': 109}</td>\n",
       "      <td>7</td>\n",
       "      <td>41</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>{'C': 111}</td>\n",
       "      <td>7</td>\n",
       "      <td>41</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>{'C': 113}</td>\n",
       "      <td>13</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>{'C': 115}</td>\n",
       "      <td>13</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>{'C': 117}</td>\n",
       "      <td>6</td>\n",
       "      <td>40</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>{'C': 119}</td>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        params  rank_test_precision_score  rank_test_recall_score  \\\n",
       "0     {'C': 1}                         60                      60   \n",
       "1     {'C': 3}                         59                      59   \n",
       "2     {'C': 5}                         58                      58   \n",
       "3     {'C': 7}                         57                      57   \n",
       "4     {'C': 9}                         56                      56   \n",
       "5    {'C': 11}                         55                      55   \n",
       "6    {'C': 13}                         54                      54   \n",
       "7    {'C': 15}                         53                      53   \n",
       "8    {'C': 17}                         52                      32   \n",
       "9    {'C': 19}                         51                      31   \n",
       "10   {'C': 21}                         50                      30   \n",
       "11   {'C': 23}                         46                      10   \n",
       "12   {'C': 25}                         46                      10   \n",
       "13   {'C': 27}                         48                      13   \n",
       "14   {'C': 29}                         49                      14   \n",
       "15   {'C': 31}                         45                       9   \n",
       "16   {'C': 33}                         19                       7   \n",
       "17   {'C': 35}                          4                       2   \n",
       "18   {'C': 37}                          1                       1   \n",
       "19   {'C': 39}                          2                       3   \n",
       "20   {'C': 41}                         15                       5   \n",
       "21   {'C': 43}                          3                       4   \n",
       "22   {'C': 45}                         22                       8   \n",
       "23   {'C': 47}                         11                       6   \n",
       "24   {'C': 49}                         20                      15   \n",
       "25   {'C': 51}                         20                      15   \n",
       "26   {'C': 53}                         12                      12   \n",
       "27   {'C': 55}                         26                      26   \n",
       "28   {'C': 57}                         39                      29   \n",
       "29   {'C': 59}                         40                      27   \n",
       "30   {'C': 61}                         40                      27   \n",
       "31   {'C': 63}                         27                      17   \n",
       "32   {'C': 65}                         27                      17   \n",
       "33   {'C': 67}                         27                      17   \n",
       "34   {'C': 69}                         27                      17   \n",
       "35   {'C': 71}                         27                      17   \n",
       "36   {'C': 73}                         27                      17   \n",
       "37   {'C': 75}                         27                      17   \n",
       "38   {'C': 77}                         27                      17   \n",
       "39   {'C': 79}                         27                      17   \n",
       "40   {'C': 81}                         43                      35   \n",
       "41   {'C': 83}                         43                      35   \n",
       "42   {'C': 85}                         36                      33   \n",
       "43   {'C': 87}                         36                      33   \n",
       "44   {'C': 89}                         42                      38   \n",
       "45   {'C': 91}                         25                      37   \n",
       "46   {'C': 93}                         38                      52   \n",
       "47   {'C': 95}                         23                      50   \n",
       "48   {'C': 97}                         23                      50   \n",
       "49   {'C': 99}                         16                      47   \n",
       "50  {'C': 101}                         16                      47   \n",
       "51  {'C': 103}                         16                      47   \n",
       "52  {'C': 105}                          7                      41   \n",
       "53  {'C': 107}                          7                      41   \n",
       "54  {'C': 109}                          7                      41   \n",
       "55  {'C': 111}                          7                      41   \n",
       "56  {'C': 113}                         13                      45   \n",
       "57  {'C': 115}                         13                      45   \n",
       "58  {'C': 117}                          6                      40   \n",
       "59  {'C': 119}                          5                      39   \n",
       "\n",
       "    rank_test_f1_score  \n",
       "0                   60  \n",
       "1                   59  \n",
       "2                   58  \n",
       "3                   57  \n",
       "4                   56  \n",
       "5                   55  \n",
       "6                   54  \n",
       "7                   53  \n",
       "8                   52  \n",
       "9                   51  \n",
       "10                  50  \n",
       "11                  46  \n",
       "12                  46  \n",
       "13                  49  \n",
       "14                  48  \n",
       "15                  45  \n",
       "16                   9  \n",
       "17                   4  \n",
       "18                   1  \n",
       "19                   2  \n",
       "20                   6  \n",
       "21                   3  \n",
       "22                  10  \n",
       "23                   5  \n",
       "24                  12  \n",
       "25                  12  \n",
       "26                   7  \n",
       "27                  18  \n",
       "28                  33  \n",
       "29                  35  \n",
       "30                  35  \n",
       "31                  21  \n",
       "32                  21  \n",
       "33                  21  \n",
       "34                  21  \n",
       "35                  21  \n",
       "36                  21  \n",
       "37                  21  \n",
       "38                  21  \n",
       "39                  21  \n",
       "40                  43  \n",
       "41                  43  \n",
       "42                  37  \n",
       "43                  37  \n",
       "44                  42  \n",
       "45                  34  \n",
       "46                  41  \n",
       "47                  39  \n",
       "48                  39  \n",
       "49                  30  \n",
       "50                  30  \n",
       "51                  30  \n",
       "52                  14  \n",
       "53                  14  \n",
       "54                  14  \n",
       "55                  14  \n",
       "56                  19  \n",
       "57                  19  \n",
       "58                  11  \n",
       "59                   8  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(gsearch1.cv_results_)[[\n",
    "       #'param_max_depth', 'param_n_estimators', \n",
    "      'params',\n",
    "       'rank_test_precision_score',\n",
    "       'rank_test_recall_score', \n",
    "       'rank_test_f1_score', \n",
    "        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import rv_discrete\n",
    "\n",
    "depths = rv_discrete(values= ([i for i in range(1,6,1)], [0.2]*5))\n",
    "n_iterations = rv_discrete(values = ([i for i in range(100,2100,50)],[0.025]*40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    "    #'C': C,\n",
    "    #'class_weight':[{0: 7},{0:8},{0:9}]\n",
    "    \"n_estimators\":n_iterations,\n",
    "    'max_depth': depths,\n",
    "}\n",
    "\n",
    "clf = GradientBoostingClassifier(random_state = 42)#RandomForestClassifier(class_weight='balanced_subsample')#SVC()\n",
    "\n",
    "gsearch1 = RandomizedSearchCV(\n",
    "    n_jobs=-1,\n",
    "    estimator=clf, \n",
    "    param_distributions=param_test1,\n",
    "    scoring= scorers,#'precision_macro',\n",
    "    verbose= True,\n",
    "    iid=True,\n",
    "    refit='precision_score',\n",
    "    n_iter= 100,\n",
    "    cv=cv)\n",
    "\n",
    "gsearch1.fit(X, Y)\n",
    "\n",
    "gsearch1.best_params_, gsearch1.best_score_"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
