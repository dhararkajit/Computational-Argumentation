{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_sent_vectors.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhararkajit/Computational-Argumentation/blob/master/word_sent_vectors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIKdYxDcmvqa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff7e1945-9bad-4bba-f913-d6088c6397cf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfFlb5rKnGuB",
        "colab_type": "code",
        "outputId": "0071a40a-1e72-4569-f21b-5a891219459e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import tokenize"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QHXNrOGUWyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!curl -Lo \"/content/drive/My Drive/Master Thesis/glove.840B.300d.zip\" http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "#!curl -Lo \"/content/drive/My Drive/Master Thesis/infersent1.pkl\" https://dl.fbaipublicfiles.com/infersent/infersent1.pkl\n",
        "#!unzip \"/content/drive/My Drive/Master Thesis/glove.840B.300d.zip\" -d \"/content/drive/My Drive/Master Thesis/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cwVP5laSaFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InferSent(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(InferSent, self).__init__()\n",
        "        self.bsize = config['bsize']\n",
        "        self.word_emb_dim = config['word_emb_dim']\n",
        "        self.enc_lstm_dim = config['enc_lstm_dim']\n",
        "        self.pool_type = config['pool_type']\n",
        "        self.dpout_model = config['dpout_model']\n",
        "        self.version = 1 if 'version' not in config else config['version']\n",
        "\n",
        "        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, 1,\n",
        "                                bidirectional=True, dropout=self.dpout_model)\n",
        "\n",
        "        assert self.version in [1, 2]\n",
        "        if self.version == 1:\n",
        "            self.bos = '<s>'\n",
        "            self.eos = '</s>'\n",
        "            self.max_pad = True\n",
        "            self.moses_tok = False\n",
        "        elif self.version == 2:\n",
        "            self.bos = '<p>'\n",
        "            self.eos = '</p>'\n",
        "            self.max_pad = False\n",
        "            self.moses_tok = True\n",
        "\n",
        "    def is_cuda(self):\n",
        "        # either all weights are on cpu or they are on gpu\n",
        "        return self.enc_lstm.bias_hh_l0.data.is_cuda\n",
        "\n",
        "    def forward(self, sent_tuple):\n",
        "        # sent_len: [max_len, ..., min_len] (bsize)\n",
        "        # sent: (seqlen x bsize x worddim)\n",
        "        sent, sent_len = sent_tuple\n",
        "\n",
        "        # Sort by length (keep idx)\n",
        "        sent_len_sorted, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)\n",
        "        sent_len_sorted = sent_len_sorted.copy()\n",
        "        idx_unsort = np.argsort(idx_sort)\n",
        "\n",
        "        idx_sort = torch.from_numpy(idx_sort).cuda() if self.is_cuda() \\\n",
        "            else torch.from_numpy(idx_sort)\n",
        "        sent = sent.index_select(1, idx_sort)\n",
        "\n",
        "        # Handling padding in Recurrent Networks\n",
        "        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len_sorted)\n",
        "        sent_output = self.enc_lstm(sent_packed)[0]  # seqlen x batch x 2*nhid\n",
        "        sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]\n",
        "\n",
        "        # Un-sort by length\n",
        "        idx_unsort = torch.from_numpy(idx_unsort).cuda() if self.is_cuda() \\\n",
        "            else torch.from_numpy(idx_unsort)\n",
        "        sent_output = sent_output.index_select(1, idx_unsort)\n",
        "\n",
        "        # Pooling\n",
        "        if self.pool_type == \"mean\":\n",
        "            sent_len = torch.FloatTensor(sent_len.copy()).unsqueeze(1).cuda()\n",
        "            emb = torch.sum(sent_output, 0).squeeze(0)\n",
        "            emb = emb / sent_len.expand_as(emb)\n",
        "        elif self.pool_type == \"max\":\n",
        "            if not self.max_pad:\n",
        "                sent_output[sent_output == 0] = -1e9\n",
        "            emb = torch.max(sent_output, 0)[0]\n",
        "            if emb.ndimension() == 3:\n",
        "                emb = emb.squeeze(0)\n",
        "                assert emb.ndimension() == 2\n",
        "\n",
        "        return emb\n",
        "\n",
        "    def set_w2v_path(self, w2v_path):\n",
        "        self.w2v_path = w2v_path\n",
        "\n",
        "    def get_word_dict(self, sentences, tokenize=True):\n",
        "        # create vocab of words\n",
        "        word_dict = {}\n",
        "        sentences = [s.split() if not tokenize else self.tokenize(s) for s in sentences]\n",
        "        for sent in sentences:\n",
        "            for word in sent:\n",
        "                if word not in word_dict:\n",
        "                    word_dict[word] = ''\n",
        "        word_dict[self.bos] = ''\n",
        "        word_dict[self.eos] = ''\n",
        "        return word_dict\n",
        "\n",
        "    def get_w2v(self, word_dict):\n",
        "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
        "        # create word_vec with w2v vectors\n",
        "        word_vec = {}\n",
        "        with open(self.w2v_path) as f:\n",
        "            for line in f:\n",
        "                word, vec = line.split(' ', 1)\n",
        "                if word in word_dict:\n",
        "                    word_vec[word] = np.fromstring(vec, sep=' ')\n",
        "        print('Found %s(/%s) words with w2v vectors' % (len(word_vec), len(word_dict)))\n",
        "        return word_vec\n",
        "\n",
        "    def get_w2v_k(self, K):\n",
        "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
        "        # create word_vec with k first w2v vectors\n",
        "        k = 0\n",
        "        word_vec = {}\n",
        "        with open(self.w2v_path) as f:\n",
        "            for line in f:\n",
        "                word, vec = line.split(' ', 1)\n",
        "                if k <= K:\n",
        "                    word_vec[word] = np.fromstring(vec, sep=' ')\n",
        "                    k += 1\n",
        "                if k > K:\n",
        "                    if word in [self.bos, self.eos]:\n",
        "                        word_vec[word] = np.fromstring(vec, sep=' ')\n",
        "\n",
        "                if k > K and all([w in word_vec for w in [self.bos, self.eos]]):\n",
        "                    break\n",
        "        return word_vec\n",
        "\n",
        "    def build_vocab(self, sentences, tokenize=True):\n",
        "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
        "        word_dict = self.get_word_dict(sentences, tokenize)\n",
        "        self.word_vec = self.get_w2v(word_dict)\n",
        "        print('Vocab size : %s' % (len(self.word_vec)))\n",
        "\n",
        "    # build w2v vocab with k most frequent words\n",
        "    def build_vocab_k_words(self, K):\n",
        "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
        "        self.word_vec = self.get_w2v_k(K)\n",
        "        print('Vocab size : %s' % (K))\n",
        "\n",
        "    def update_vocab(self, sentences, tokenize=True):\n",
        "        assert hasattr(self, 'w2v_path'), 'warning : w2v path not set'\n",
        "        assert hasattr(self, 'word_vec'), 'build_vocab before updating it'\n",
        "        word_dict = self.get_word_dict(sentences, tokenize)\n",
        "\n",
        "        # keep only new words\n",
        "        for word in self.word_vec:\n",
        "            if word in word_dict:\n",
        "                del word_dict[word]\n",
        "\n",
        "        # udpate vocabulary\n",
        "        if word_dict:\n",
        "            new_word_vec = self.get_w2v(word_dict)\n",
        "            self.word_vec.update(new_word_vec)\n",
        "        else:\n",
        "            new_word_vec = []\n",
        "        print('New vocab size : %s (added %s words)'% (len(self.word_vec), len(new_word_vec)))\n",
        "\n",
        "    def get_batch(self, batch):\n",
        "        # sent in batch in decreasing order of lengths\n",
        "        # batch: (bsize, max_len, word_dim)\n",
        "        embed = np.zeros((len(batch[0]), len(batch), self.word_emb_dim))\n",
        "\n",
        "        for i in range(len(batch)):\n",
        "            for j in range(len(batch[i])):\n",
        "                embed[j, i, :] = self.word_vec[batch[i][j]]\n",
        "\n",
        "        return torch.FloatTensor(embed)\n",
        "\n",
        "    def tokenize(self, s):\n",
        "        from nltk.tokenize import word_tokenize\n",
        "        if self.moses_tok:\n",
        "            s = ' '.join(word_tokenize(s))\n",
        "            s = s.replace(\" n't \", \"n 't \")  # HACK to get ~MOSES tokenization\n",
        "            return s.split()\n",
        "        else:\n",
        "            return word_tokenize(s)\n",
        "\n",
        "    def prepare_samples(self, sentences, bsize, tokenize, verbose):\n",
        "        sentences = [[self.bos] + s.split() + [self.eos] if not tokenize else\n",
        "                     [self.bos] + self.tokenize(s) + [self.eos] for s in sentences]\n",
        "        n_w = np.sum([len(x) for x in sentences])\n",
        "\n",
        "        # filters words without w2v vectors\n",
        "        for i in range(len(sentences)):\n",
        "            s_f = [word for word in sentences[i] if word in self.word_vec]\n",
        "            if not s_f:\n",
        "                import warnings\n",
        "                warnings.warn('No words in \"%s\" (idx=%s) have w2v vectors. \\\n",
        "                               Replacing by \"</s>\"..' % (sentences[i], i))\n",
        "                s_f = [self.eos]\n",
        "            sentences[i] = s_f\n",
        "\n",
        "        lengths = np.array([len(s) for s in sentences])\n",
        "        n_wk = np.sum(lengths)\n",
        "        if verbose:\n",
        "            print('Nb words kept : %s/%s (%.1f%s)' % (\n",
        "                        n_wk, n_w, 100.0 * n_wk / n_w, '%'))\n",
        "\n",
        "        # sort by decreasing length\n",
        "        lengths, idx_sort = np.sort(lengths)[::-1], np.argsort(-lengths)\n",
        "        sentences = np.array(sentences)[idx_sort]\n",
        "\n",
        "        return sentences, lengths, idx_sort\n",
        "\n",
        "    def encode(self, sentences, bsize=64, tokenize=True, verbose=False):\n",
        "        tic = time.time()\n",
        "        sentences, lengths, idx_sort = self.prepare_samples(\n",
        "                        sentences, bsize, tokenize, verbose)\n",
        "\n",
        "        embeddings = []\n",
        "        for stidx in range(0, len(sentences), bsize):\n",
        "            batch = self.get_batch(sentences[stidx:stidx + bsize])\n",
        "            if self.is_cuda():\n",
        "                batch = batch.cuda()\n",
        "            with torch.no_grad():\n",
        "                batch = self.forward((batch, lengths[stidx:stidx + bsize])).data.cpu().numpy()\n",
        "            embeddings.append(batch)\n",
        "        embeddings = np.vstack(embeddings)\n",
        "\n",
        "        # unsort\n",
        "        idx_unsort = np.argsort(idx_sort)\n",
        "        embeddings = embeddings[idx_unsort]\n",
        "\n",
        "        if verbose:\n",
        "            print('Speed : %.1f sentences/s (%s mode, bsize=%s)' % (\n",
        "                    len(embeddings)/(time.time()-tic),\n",
        "                    'gpu' if self.is_cuda() else 'cpu', bsize))\n",
        "        return embeddings\n",
        "\n",
        "    def visualize(self, sent, tokenize=True):\n",
        "\n",
        "        sent = sent.split() if not tokenize else self.tokenize(sent)\n",
        "        sent = [[self.bos] + [word for word in sent if word in self.word_vec] + [self.eos]]\n",
        "\n",
        "        if ' '.join(sent[0]) == '%s %s' % (self.bos, self.eos):\n",
        "            import warnings\n",
        "            warnings.warn('No words in \"%s\" have w2v vectors. Replacing \\\n",
        "                           by \"%s %s\"..' % (sent, self.bos, self.eos))\n",
        "        batch = self.get_batch(sent)\n",
        "\n",
        "        if self.is_cuda():\n",
        "            batch = batch.cuda()\n",
        "        output = self.enc_lstm(batch)[0]\n",
        "        output, idxs = torch.max(output, 0)\n",
        "        # output, idxs = output.squeeze(), idxs.squeeze()\n",
        "        idxs = idxs.data.cpu().numpy()\n",
        "        argmaxs = [np.sum((idxs == k)) for k in range(len(sent[0]))]\n",
        "\n",
        "        # visualize model\n",
        "        import matplotlib.pyplot as plt\n",
        "        plt.figure(figsize=(12,12))\n",
        "        x = range(len(sent[0]))\n",
        "        y = [100.0 * n / np.sum(argmaxs) for n in argmaxs]\n",
        "        plt.xticks(x, sent[0], rotation=45)\n",
        "        plt.bar(x, y)\n",
        "        plt.ylabel('%')\n",
        "        plt.title('Visualisation of words importance')\n",
        "        plt.show()\n",
        "\n",
        "        return output, idxs, argmaxs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWolj8Z0SaI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_version = 1\n",
        "MODEL_PATH = \"/content/drive/My Drive/Master Thesis/infersent%s.pkl\" % model_version\n",
        "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
        "                'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvJH6SHsSaMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = InferSent(params_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ovOx6hhSaO9",
        "colab_type": "code",
        "outputId": "e2c8691f-bf09-4b9b-a523-1a919df90d85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.load_state_dict(torch.load(MODEL_PATH))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNmtja9XSaDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W2V_PATH = '/content/drive/My Drive/Master Thesis/glove.840B.300d.txt'\n",
        "model.set_w2v_path(W2V_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaO7U00Aago2",
        "colab_type": "code",
        "outputId": "37791b51-833a-44ef-826d-8a57f58572b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.build_vocab_k_words(K=100000)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size : 100000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1oZQOvDpO8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "argument_pairs = pd.read_json('/content/drive/My Drive/Master Thesis/arg-counter.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg941SParA1v",
        "colab_type": "code",
        "outputId": "ac953cf0-5748-44a2-e2c1-7c4b2a34e8bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "argument_pairs.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>selftext</th>\n",
              "      <th>body_y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>America's Education System is very flawed.</th>\n",
              "      <td>throughout the whole republican democratic deb...</td>\n",
              "      <td>throughout the last few years the u .s . has ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Announcing Best of CMV 2015</th>\n",
              "      <td>we are having a best of contest ! there are ca...</td>\n",
              "      <td>i liked the extremely informative nature of th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Boneless wings are effectively superior to their boned counterparts.</th>\n",
              "      <td>those who feast on boned wings are either moti...</td>\n",
              "      <td>i used to agree with you fully on this as what...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CMV (politics): Hillary as president will likely lead to more war than Trump</th>\n",
              "      <td>i don t like trump . hillary seems by far less...</td>\n",
              "      <td>you are aware that the libyan rebels were bein...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CMV - I believe human civilization is doomed. *PLEASE* change my view.</th>\n",
              "      <td>well i hate to be a downer guys but i don t th...</td>\n",
              "      <td>worse and worse news just because the news is...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                             selftext                                             body_y\n",
              "America's Education System is very flawed.          throughout the whole republican democratic deb...   throughout the last few years the u .s . has ...\n",
              "Announcing Best of CMV 2015                         we are having a best of contest ! there are ca...  i liked the extremely informative nature of th...\n",
              "Boneless wings are effectively superior to thei...  those who feast on boned wings are either moti...  i used to agree with you fully on this as what...\n",
              "CMV (politics): Hillary as president will likel...  i don t like trump . hillary seems by far less...  you are aware that the libyan rebels were bein...\n",
              "CMV - I believe human civilization is doomed. *...  well i hate to be a downer guys but i don t th...   worse and worse news just because the news is..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcbKdu7TrPDD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "argument_pairs['selftext'] = argument_pairs['selftext'].apply(lambda x : tokenize.sent_tokenize(x))\n",
        "argument_pairs['body_y'] = argument_pairs['body_y'].apply(lambda x : tokenize.sent_tokenize(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "za6nw_viVEkE",
        "colab_type": "code",
        "outputId": "e517563e-0c7f-48a1-9aa8-6bbcd330343c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "embeddings = model.encode(argument_pairs.loc[\"America's Education System is very flawed.\",'body_y'], bsize=128, tokenize=False, verbose=True)\n",
        "print('nb sentences encoded : {0}'.format(len(embeddings)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nb words kept : 105/106 (99.1%)\n",
            "Speed : 18.4 sentences/s (cpu mode, bsize=128)\n",
            "nb sentences encoded : 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLxWIIJyag9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "argument_pairs['op_embedding'] = argument_pairs['selftext'].apply(lambda x : model.encode(x, bsize=128, tokenize=False, verbose=False))\n",
        "argument_pairs['comment_embedding'] = argument_pairs['body_y'].apply(lambda x : model.encode(x, bsize=128, tokenize=False, verbose=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmAUL3OiVyqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#argument_pairs.to_json('/content/drive/My Drive/Master Thesis/argument_pairs_embdng.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a2gxR2jagyK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}